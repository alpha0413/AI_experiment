{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxHb4KGqW0kd"
   },
   "source": [
    "Google Driveを使えるようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1759728748835,
     "user": {
      "displayName": "Mihiro OTAKI",
      "userId": "02683145051404329861"
     },
     "user_tz": -540
    },
    "id": "OgQBzkppWzF-"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 19120,
     "status": "error",
     "timestamp": 1759729043413,
     "user": {
      "displayName": "Mihiro OTAKI",
      "userId": "02683145051404329861"
     },
     "user_tz": -540
    },
    "id": "x69n9V55sztq",
    "outputId": "1260aa37-d630-4704-c37b-5376cd78821e"
   },
   "outputs": [],
   "source": [
    "# drive.mount('/content/drive',  force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbiUq94bXFOO"
   },
   "source": [
    "カレントディレクトリを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1067,
     "status": "ok",
     "timestamp": 1569253451966,
     "user": {
      "displayName": "Mantaro Yamada",
      "photoUrl": "",
      "userId": "14309209972069666827"
     },
     "user_tz": -540
    },
    "id": "f3SX7-qOWXlJ",
    "outputId": "74772a82-db11-420f-d277-297406007da2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user_kodai/AI_experiment/cv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5fr5CkzW8nV"
   },
   "source": [
    "対象のディレクトリに移動する（\"/content/drive/My Drive/\" があなたのドライブのホームディレクトリ）\n",
    "\n",
    "この例の場合，ホームにAI_Experimentというフォルダを作り，その下にダウンロードしたフォルダを配置している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1729,
     "status": "ok",
     "timestamp": 1569253459596,
     "user": {
      "displayName": "Mantaro Yamada",
      "photoUrl": "",
      "userId": "14309209972069666827"
     },
     "user_tz": -540
    },
    "id": "oD7f7_lOs18H",
    "outputId": "d9f106dd-bfc7-4475-cc06-79f1093e0fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/AI_Experiment/section4\n"
     ]
    }
   ],
   "source": [
    "# cd /content/drive/My Drive/AI_Experiment/cv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 4989,
     "status": "ok",
     "timestamp": 1569253467993,
     "user": {
      "displayName": "Mantaro Yamada",
      "photoUrl": "",
      "userId": "14309209972069666827"
     },
     "user_tz": -540
    },
    "id": "Vnx69X3htCqt",
    "outputId": "cee4854e-f72c-49af-ee3b-eff2bcdb0cf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_db.py  network.py             search.py          train_mnist_cnn.py\n",
      "\u001b[0m\u001b[01;34mdata\u001b[0m/         network_db.py          test_cifar10.py    train_mnist_mlp.py\n",
      "dataset.py    \u001b[01;34mresult\u001b[0m/                test_mnist_mlp.py\n",
      "\u001b[01;35mimage.png\u001b[0m     run_python_code.ipynb  train_cifar10.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csNmo2pwXKPv"
   },
   "source": [
    "試しにPythonコードを実行する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演習4.2.1\n",
    "GPUで実行せよ、とありますが基本的にローカルで実行しているため、ここでは-gをつけていません。この課題についてはgoogle colabにおいても実行しているため、それを持ってこの課題を実行したということにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "executionInfo": {
     "elapsed": 196929,
     "status": "ok",
     "timestamp": 1569252482156,
     "user": {
      "displayName": "Mantaro Yamada",
      "photoUrl": "",
      "userId": "14309209972069666827"
     },
     "user_tz": -540
    },
    "id": "aJUDjSAZtZv3",
    "outputId": "f8d4d3d9-940c-4416-b794-c8dc5a195389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "# unit: 1000\n",
      "# Minibatch-size: 100\n",
      "# epoch: 20\n",
      "\n",
      "[epoch 1] loss: 1048.426\n",
      "[epoch 2] loss: 418.917\n",
      "[epoch 3] loss: 255.138\n",
      "[epoch 4] loss: 211.331\n",
      "[epoch 5] loss: 190.312\n",
      "[epoch 6] loss: 176.663\n",
      "[epoch 7] loss: 166.568\n",
      "[epoch 8] loss: 158.047\n",
      "[epoch 9] loss: 150.737\n",
      "[epoch 10] loss: 144.238\n",
      "[epoch 11] loss: 138.215\n",
      "[epoch 12] loss: 132.612\n",
      "[epoch 13] loss: 127.557\n",
      "[epoch 14] loss: 122.620\n",
      "[epoch 15] loss: 117.828\n",
      "[epoch 16] loss: 113.570\n",
      "[epoch 17] loss: 109.072\n",
      "[epoch 18] loss: 105.403\n",
      "[epoch 19] loss: 101.596\n",
      "[epoch 20] loss: 98.082\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "!python train_mnist_mlp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演習4.2.2\n",
    "2と書いた画像を用意した。(2.png, 2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演習4.2.3\n",
    "2.pngは3と誤認されたが、2-1.pngは2であると判定された。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "\n",
      "Predicted label : 3\n"
     ]
    }
   ],
   "source": [
    "!python test_mnist_mlp.py -i 2.png -m result/model_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "\n",
      "Predicted label : 2\n"
     ]
    }
   ],
   "source": [
    "!python test_mnist_mlp.py -i 2-1.png -m result/model_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演習4.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "\n",
      "Predicted label : [[3, 2, 8]]\n"
     ]
    }
   ],
   "source": [
    "!python test_mnist_mlp.py -i 2.png -m result/model_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "\n",
      "Predicted label : [[2, 3, 8]]\n"
     ]
    }
   ],
   "source": [
    "!python test_mnist_mlp.py -i 2-1.png -m result/model_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演習4.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "# unit: 3\n",
      "# Minibatch-size: 100\n",
      "# epoch: 20\n",
      "\n",
      "[epoch 1] loss: 1237.682\n",
      "[epoch 2] loss: 1149.193\n",
      "[epoch 3] loss: 1093.049\n",
      "[epoch 4] loss: 1053.536\n",
      "[epoch 5] loss: 971.041\n",
      "[epoch 6] loss: 906.667\n",
      "[epoch 7] loss: 862.390\n",
      "[epoch 8] loss: 829.395\n",
      "[epoch 9] loss: 794.530\n",
      "[epoch 10] loss: 754.861\n",
      "[epoch 11] loss: 722.358\n",
      "[epoch 12] loss: 698.598\n",
      "[epoch 13] loss: 680.353\n",
      "[epoch 14] loss: 664.933\n",
      "[epoch 15] loss: 650.200\n",
      "[epoch 16] loss: 634.555\n",
      "[epoch 17] loss: 616.033\n",
      "[epoch 18] loss: 593.398\n",
      "[epoch 19] loss: 569.450\n",
      "[epoch 20] loss: 546.576\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "!python train_mnist_mlp.py -u 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "\n",
      "Predicted label : [[5, 3, 8]]\n"
     ]
    }
   ],
   "source": [
    "!python test_mnist_mlp.py -i 2.png -u 3 -m result/model_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演習4.2.5\n",
    "下には中間ノード数が3の結果のみを示しているが、実際にはgoogle colabで10000も行っている。最終的な精度についてはresultフォルダの中のaccuracy_mnist_mlp_u3.pngとaccuracy_mnist_mlp_u10000.pngにそれぞれ表示している。ここから、中間ノードが多い方が精度が高くなる傾向にあることが確認できた。実行時間についてどちらもgoogle colabで「%%time」を用いて比較したところ、中間ノード数が3の時には3分4秒なのに対して中間ノード数が10000の時には9分12秒かかっていたので、中間ノード数が多いほど時間がかかる傾向にあることを確認できた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "# unit: 3\n",
      "# Minibatch-size: 100\n",
      "# epoch: 20\n",
      "\n",
      "[epoch 1] loss: 1193.152\n",
      "[epoch 2] loss: 1089.746\n",
      "[epoch 3] loss: 1015.815\n",
      "[epoch 4] loss: 939.771\n",
      "[epoch 5] loss: 867.075\n",
      "[epoch 6] loss: 761.037\n",
      "[epoch 7] loss: 680.485\n",
      "[epoch 8] loss: 625.641\n",
      "[epoch 9] loss: 581.772\n",
      "[epoch 10] loss: 544.800\n",
      "[epoch 11] loss: 516.476\n",
      "[epoch 12] loss: 496.497\n",
      "[epoch 13] loss: 482.178\n",
      "[epoch 14] loss: 470.922\n",
      "[epoch 15] loss: 461.703\n",
      "[epoch 16] loss: 453.639\n",
      "[epoch 17] loss: 446.302\n",
      "[epoch 18] loss: 440.641\n",
      "[epoch 19] loss: 435.194\n",
      "[epoch 20] loss: 430.582\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "!python train_mnist_mlp.py -u 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演習4.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "# Minibatch-size: 100\n",
      "# epoch: 20\n",
      "\n",
      "[epoch 1] loss: 1106.654\n",
      "[epoch 2] loss: 253.646\n",
      "[epoch 3] loss: 193.496\n",
      "[epoch 4] loss: 172.663\n",
      "[epoch 5] loss: 156.827\n",
      "[epoch 6] loss: 142.497\n",
      "[epoch 7] loss: 129.762\n",
      "[epoch 8] loss: 118.714\n",
      "[epoch 9] loss: 109.026\n",
      "[epoch 10] loss: 100.375\n",
      "[epoch 11] loss: 93.865\n",
      "[epoch 12] loss: 87.756\n",
      "[epoch 13] loss: 82.325\n",
      "[epoch 14] loss: 77.904\n",
      "[epoch 15] loss: 73.329\n",
      "[epoch 16] loss: 69.325\n",
      "[epoch 17] loss: 66.895\n",
      "[epoch 18] loss: 63.050\n",
      "[epoch 19] loss: 60.850\n",
      "[epoch 20] loss: 58.016\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "!python train_mnist_cnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnnの方が少し精度が高いことが確認できる。しかもこれは中間層を10000にした場合との比較なので、精度が良いことが確認できた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演習4.2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "# Minibatch-size: 100\n",
      "# epoch: 500\n",
      "\n",
      "[epoch 1] loss: 32.307\n",
      "[epoch 2] loss: 32.252\n",
      "[epoch 3] loss: 32.189\n",
      "[epoch 4] loss: 32.132\n",
      "[epoch 5] loss: 32.107\n",
      "[epoch 6] loss: 32.068\n",
      "[epoch 7] loss: 32.003\n",
      "[epoch 8] loss: 31.983\n",
      "[epoch 9] loss: 31.916\n",
      "[epoch 10] loss: 31.859\n",
      "[epoch 11] loss: 31.813\n",
      "[epoch 12] loss: 31.725\n",
      "[epoch 13] loss: 31.672\n",
      "[epoch 14] loss: 31.559\n",
      "[epoch 15] loss: 31.435\n",
      "[epoch 16] loss: 31.348\n",
      "[epoch 17] loss: 31.180\n",
      "[epoch 18] loss: 31.019\n",
      "[epoch 19] loss: 30.915\n",
      "[epoch 20] loss: 30.807\n",
      "[epoch 21] loss: 30.638\n",
      "[epoch 22] loss: 30.506\n",
      "[epoch 23] loss: 30.406\n",
      "[epoch 24] loss: 30.294\n",
      "[epoch 25] loss: 30.341\n",
      "[epoch 26] loss: 30.235\n",
      "[epoch 27] loss: 30.196\n",
      "[epoch 28] loss: 30.197\n",
      "[epoch 29] loss: 30.160\n",
      "[epoch 30] loss: 29.870\n",
      "[epoch 31] loss: 29.906\n",
      "[epoch 32] loss: 29.960\n",
      "[epoch 33] loss: 29.863\n",
      "[epoch 34] loss: 29.780\n",
      "[epoch 35] loss: 29.696\n",
      "[epoch 36] loss: 29.627\n",
      "[epoch 37] loss: 29.675\n",
      "[epoch 38] loss: 29.609\n",
      "[epoch 39] loss: 29.530\n",
      "[epoch 40] loss: 29.421\n",
      "[epoch 41] loss: 29.192\n",
      "[epoch 42] loss: 29.216\n",
      "[epoch 43] loss: 29.129\n",
      "[epoch 44] loss: 28.894\n",
      "[epoch 45] loss: 28.688\n",
      "[epoch 46] loss: 28.510\n",
      "[epoch 47] loss: 28.344\n",
      "[epoch 48] loss: 28.280\n",
      "[epoch 49] loss: 27.905\n",
      "[epoch 50] loss: 27.820\n",
      "[epoch 51] loss: 27.599\n",
      "[epoch 52] loss: 27.343\n",
      "[epoch 53] loss: 27.249\n",
      "[epoch 54] loss: 27.161\n",
      "[epoch 55] loss: 27.123\n",
      "[epoch 56] loss: 27.013\n",
      "[epoch 57] loss: 26.898\n",
      "[epoch 58] loss: 26.715\n",
      "[epoch 59] loss: 26.584\n",
      "[epoch 60] loss: 26.403\n",
      "[epoch 61] loss: 26.301\n",
      "[epoch 62] loss: 26.333\n",
      "[epoch 63] loss: 26.356\n",
      "[epoch 64] loss: 26.245\n",
      "[epoch 65] loss: 25.954\n",
      "[epoch 66] loss: 26.029\n",
      "[epoch 67] loss: 26.218\n",
      "[epoch 68] loss: 25.760\n",
      "[epoch 69] loss: 25.736\n",
      "[epoch 70] loss: 25.732\n",
      "[epoch 71] loss: 25.803\n",
      "[epoch 72] loss: 25.698\n",
      "[epoch 73] loss: 25.518\n",
      "[epoch 74] loss: 25.507\n",
      "[epoch 75] loss: 25.347\n",
      "[epoch 76] loss: 25.145\n",
      "[epoch 77] loss: 25.190\n",
      "[epoch 78] loss: 25.032\n",
      "[epoch 79] loss: 25.061\n",
      "[epoch 80] loss: 24.912\n",
      "[epoch 81] loss: 24.860\n",
      "[epoch 82] loss: 24.736\n",
      "[epoch 83] loss: 24.601\n",
      "[epoch 84] loss: 24.646\n",
      "[epoch 85] loss: 24.756\n",
      "[epoch 86] loss: 24.692\n",
      "[epoch 87] loss: 24.296\n",
      "[epoch 88] loss: 24.338\n",
      "[epoch 89] loss: 24.380\n",
      "[epoch 90] loss: 23.923\n",
      "[epoch 91] loss: 23.809\n",
      "[epoch 92] loss: 23.717\n",
      "[epoch 93] loss: 23.710\n",
      "[epoch 94] loss: 23.603\n",
      "[epoch 95] loss: 23.308\n",
      "[epoch 96] loss: 23.323\n",
      "[epoch 97] loss: 23.309\n",
      "[epoch 98] loss: 23.160\n",
      "[epoch 99] loss: 23.283\n",
      "[epoch 100] loss: 23.102\n",
      "[epoch 101] loss: 22.996\n",
      "[epoch 102] loss: 22.891\n",
      "[epoch 103] loss: 22.696\n",
      "[epoch 104] loss: 22.534\n",
      "[epoch 105] loss: 22.592\n",
      "[epoch 106] loss: 22.408\n",
      "[epoch 107] loss: 22.430\n",
      "[epoch 108] loss: 22.193\n",
      "[epoch 109] loss: 22.219\n",
      "[epoch 110] loss: 22.019\n",
      "[epoch 111] loss: 21.895\n",
      "[epoch 112] loss: 22.023\n",
      "[epoch 113] loss: 21.745\n",
      "[epoch 114] loss: 21.605\n",
      "[epoch 115] loss: 21.526\n",
      "[epoch 116] loss: 21.762\n",
      "[epoch 117] loss: 21.654\n",
      "[epoch 118] loss: 21.398\n",
      "[epoch 119] loss: 21.142\n",
      "[epoch 120] loss: 20.949\n",
      "[epoch 121] loss: 20.989\n",
      "[epoch 122] loss: 20.968\n",
      "[epoch 123] loss: 20.876\n",
      "[epoch 124] loss: 20.656\n",
      "[epoch 125] loss: 20.831\n",
      "[epoch 126] loss: 20.769\n",
      "[epoch 127] loss: 20.442\n",
      "[epoch 128] loss: 20.456\n",
      "[epoch 129] loss: 20.334\n",
      "[epoch 130] loss: 20.392\n",
      "[epoch 131] loss: 20.273\n",
      "[epoch 132] loss: 19.963\n",
      "[epoch 133] loss: 20.020\n",
      "[epoch 134] loss: 19.669\n",
      "[epoch 135] loss: 19.786\n",
      "[epoch 136] loss: 19.681\n",
      "[epoch 137] loss: 19.636\n",
      "[epoch 138] loss: 19.623\n",
      "[epoch 139] loss: 19.261\n",
      "[epoch 140] loss: 19.295\n",
      "[epoch 141] loss: 19.285\n",
      "[epoch 142] loss: 19.127\n",
      "[epoch 143] loss: 19.367\n",
      "[epoch 144] loss: 19.965\n",
      "[epoch 145] loss: 19.665\n",
      "[epoch 146] loss: 18.905\n",
      "[epoch 147] loss: 18.921\n",
      "[epoch 148] loss: 19.054\n",
      "[epoch 149] loss: 19.039\n",
      "[epoch 150] loss: 18.922\n",
      "[epoch 151] loss: 19.069\n",
      "[epoch 152] loss: 18.618\n",
      "[epoch 153] loss: 18.632\n",
      "[epoch 154] loss: 18.611\n",
      "[epoch 155] loss: 18.499\n",
      "[epoch 156] loss: 18.408\n",
      "[epoch 157] loss: 18.309\n",
      "[epoch 158] loss: 18.274\n",
      "[epoch 159] loss: 18.343\n",
      "[epoch 160] loss: 18.127\n",
      "[epoch 161] loss: 18.515\n",
      "[epoch 162] loss: 18.093\n",
      "[epoch 163] loss: 18.052\n",
      "[epoch 164] loss: 17.904\n",
      "[epoch 165] loss: 17.870\n",
      "[epoch 166] loss: 17.834\n",
      "[epoch 167] loss: 17.764\n",
      "[epoch 168] loss: 17.814\n",
      "[epoch 169] loss: 17.653\n",
      "[epoch 170] loss: 17.593\n",
      "[epoch 171] loss: 17.245\n",
      "[epoch 172] loss: 17.447\n",
      "[epoch 173] loss: 17.155\n",
      "[epoch 174] loss: 17.141\n",
      "[epoch 175] loss: 17.166\n",
      "[epoch 176] loss: 17.531\n",
      "[epoch 177] loss: 17.145\n",
      "[epoch 178] loss: 17.386\n",
      "[epoch 179] loss: 17.153\n",
      "[epoch 180] loss: 16.691\n",
      "[epoch 181] loss: 16.898\n",
      "[epoch 182] loss: 16.691\n",
      "[epoch 183] loss: 16.700\n",
      "[epoch 184] loss: 16.585\n",
      "[epoch 185] loss: 17.275\n",
      "[epoch 186] loss: 17.146\n",
      "[epoch 187] loss: 16.830\n",
      "[epoch 188] loss: 16.787\n",
      "[epoch 189] loss: 16.717\n",
      "[epoch 190] loss: 16.520\n",
      "[epoch 191] loss: 16.870\n",
      "[epoch 192] loss: 16.615\n",
      "[epoch 193] loss: 16.777\n",
      "[epoch 194] loss: 16.220\n",
      "[epoch 195] loss: 16.202\n",
      "[epoch 196] loss: 16.167\n",
      "[epoch 197] loss: 16.261\n",
      "[epoch 198] loss: 16.365\n",
      "[epoch 199] loss: 16.128\n",
      "[epoch 200] loss: 16.184\n",
      "[epoch 201] loss: 15.823\n",
      "[epoch 202] loss: 15.789\n",
      "[epoch 203] loss: 16.634\n",
      "[epoch 204] loss: 15.815\n",
      "[epoch 205] loss: 15.963\n",
      "[epoch 206] loss: 15.341\n",
      "[epoch 207] loss: 15.808\n",
      "[epoch 208] loss: 15.532\n",
      "[epoch 209] loss: 15.414\n",
      "[epoch 210] loss: 15.325\n",
      "[epoch 211] loss: 15.194\n",
      "[epoch 212] loss: 15.164\n",
      "[epoch 213] loss: 15.080\n",
      "[epoch 214] loss: 15.066\n",
      "[epoch 215] loss: 15.092\n",
      "[epoch 216] loss: 14.954\n",
      "[epoch 217] loss: 14.995\n",
      "[epoch 218] loss: 14.747\n",
      "[epoch 219] loss: 14.943\n",
      "[epoch 220] loss: 15.079\n",
      "[epoch 221] loss: 15.003\n",
      "[epoch 222] loss: 15.342\n",
      "[epoch 223] loss: 14.940\n",
      "[epoch 224] loss: 14.878\n",
      "[epoch 225] loss: 14.597\n",
      "[epoch 226] loss: 14.909\n",
      "[epoch 227] loss: 14.757\n",
      "[epoch 228] loss: 14.412\n",
      "[epoch 229] loss: 14.073\n",
      "[epoch 230] loss: 14.229\n",
      "[epoch 231] loss: 14.345\n",
      "[epoch 232] loss: 14.363\n",
      "[epoch 233] loss: 14.115\n",
      "[epoch 234] loss: 14.202\n",
      "[epoch 235] loss: 14.190\n",
      "[epoch 236] loss: 14.222\n",
      "[epoch 237] loss: 13.861\n",
      "[epoch 238] loss: 13.802\n",
      "[epoch 239] loss: 13.715\n",
      "[epoch 240] loss: 14.012\n",
      "[epoch 241] loss: 13.706\n",
      "[epoch 242] loss: 13.689\n",
      "[epoch 243] loss: 13.924\n",
      "[epoch 244] loss: 13.627\n",
      "[epoch 245] loss: 13.709\n",
      "[epoch 246] loss: 13.625\n",
      "[epoch 247] loss: 13.889\n",
      "[epoch 248] loss: 13.542\n",
      "[epoch 249] loss: 13.224\n",
      "[epoch 250] loss: 12.992\n",
      "[epoch 251] loss: 12.960\n",
      "[epoch 252] loss: 13.251\n",
      "[epoch 253] loss: 13.316\n",
      "[epoch 254] loss: 12.867\n",
      "[epoch 255] loss: 13.186\n",
      "[epoch 256] loss: 12.988\n",
      "[epoch 257] loss: 12.967\n",
      "[epoch 258] loss: 12.767\n",
      "[epoch 259] loss: 12.780\n",
      "[epoch 260] loss: 12.762\n",
      "[epoch 261] loss: 12.758\n",
      "[epoch 262] loss: 12.855\n",
      "[epoch 263] loss: 12.656\n",
      "[epoch 264] loss: 12.595\n",
      "[epoch 265] loss: 12.324\n",
      "[epoch 266] loss: 12.303\n",
      "[epoch 267] loss: 12.325\n",
      "[epoch 268] loss: 12.142\n",
      "[epoch 269] loss: 11.877\n",
      "[epoch 270] loss: 12.207\n",
      "[epoch 271] loss: 12.226\n",
      "[epoch 272] loss: 11.823\n",
      "[epoch 273] loss: 12.243\n",
      "[epoch 274] loss: 11.845\n",
      "[epoch 275] loss: 11.968\n",
      "[epoch 276] loss: 12.041\n",
      "[epoch 277] loss: 12.222\n",
      "[epoch 278] loss: 12.028\n",
      "[epoch 279] loss: 11.791\n",
      "[epoch 280] loss: 11.889\n",
      "[epoch 281] loss: 11.608\n",
      "[epoch 282] loss: 11.340\n",
      "[epoch 283] loss: 11.598\n",
      "[epoch 284] loss: 11.841\n",
      "[epoch 285] loss: 11.646\n",
      "[epoch 286] loss: 11.554\n",
      "[epoch 287] loss: 11.360\n",
      "[epoch 288] loss: 11.109\n",
      "[epoch 289] loss: 11.728\n",
      "[epoch 290] loss: 11.115\n",
      "[epoch 291] loss: 11.100\n",
      "[epoch 292] loss: 10.726\n",
      "[epoch 293] loss: 10.785\n",
      "[epoch 294] loss: 10.902\n",
      "[epoch 295] loss: 10.580\n",
      "[epoch 296] loss: 10.825\n",
      "[epoch 297] loss: 10.854\n",
      "[epoch 298] loss: 10.822\n",
      "[epoch 299] loss: 10.509\n",
      "[epoch 300] loss: 10.561\n",
      "[epoch 301] loss: 10.761\n",
      "[epoch 302] loss: 10.563\n",
      "[epoch 303] loss: 10.330\n",
      "[epoch 304] loss: 10.122\n",
      "[epoch 305] loss: 10.339\n",
      "[epoch 306] loss: 10.227\n",
      "[epoch 307] loss: 9.846\n",
      "[epoch 308] loss: 10.038\n",
      "[epoch 309] loss: 9.835\n",
      "[epoch 310] loss: 10.170\n",
      "[epoch 311] loss: 9.811\n",
      "[epoch 312] loss: 9.999\n",
      "[epoch 313] loss: 9.873\n",
      "[epoch 314] loss: 10.074\n",
      "[epoch 315] loss: 10.300\n",
      "[epoch 316] loss: 10.009\n",
      "[epoch 317] loss: 9.790\n",
      "[epoch 318] loss: 9.859\n",
      "[epoch 319] loss: 9.457\n",
      "[epoch 320] loss: 9.329\n",
      "[epoch 321] loss: 9.293\n",
      "[epoch 322] loss: 9.464\n",
      "[epoch 323] loss: 9.174\n",
      "[epoch 324] loss: 9.210\n",
      "[epoch 325] loss: 9.129\n",
      "[epoch 326] loss: 9.172\n",
      "[epoch 327] loss: 8.998\n",
      "[epoch 328] loss: 9.147\n",
      "[epoch 329] loss: 9.154\n",
      "[epoch 330] loss: 9.148\n",
      "[epoch 331] loss: 8.998\n",
      "[epoch 332] loss: 9.309\n",
      "[epoch 333] loss: 8.760\n",
      "[epoch 334] loss: 8.426\n",
      "[epoch 335] loss: 8.725\n",
      "[epoch 336] loss: 8.493\n",
      "[epoch 337] loss: 8.847\n",
      "[epoch 338] loss: 8.530\n",
      "[epoch 339] loss: 8.740\n",
      "[epoch 340] loss: 8.704\n",
      "[epoch 341] loss: 8.416\n",
      "[epoch 342] loss: 8.602\n",
      "[epoch 343] loss: 8.724\n",
      "[epoch 344] loss: 8.208\n",
      "[epoch 345] loss: 8.476\n",
      "[epoch 346] loss: 8.567\n",
      "[epoch 347] loss: 8.452\n",
      "[epoch 348] loss: 9.046\n",
      "[epoch 349] loss: 8.233\n",
      "[epoch 350] loss: 8.026\n",
      "[epoch 351] loss: 7.993\n",
      "[epoch 352] loss: 7.936\n",
      "[epoch 353] loss: 8.173\n",
      "[epoch 354] loss: 8.090\n",
      "[epoch 355] loss: 7.610\n",
      "[epoch 356] loss: 7.651\n",
      "[epoch 357] loss: 7.602\n",
      "[epoch 358] loss: 7.746\n",
      "[epoch 359] loss: 7.391\n",
      "[epoch 360] loss: 7.724\n",
      "[epoch 361] loss: 7.618\n",
      "[epoch 362] loss: 7.283\n",
      "[epoch 363] loss: 7.312\n",
      "[epoch 364] loss: 7.675\n",
      "[epoch 365] loss: 7.361\n",
      "[epoch 366] loss: 7.104\n",
      "[epoch 367] loss: 7.138\n",
      "[epoch 368] loss: 7.356\n",
      "[epoch 369] loss: 7.363\n",
      "[epoch 370] loss: 7.080\n",
      "[epoch 371] loss: 7.427\n",
      "[epoch 372] loss: 7.316\n",
      "[epoch 373] loss: 7.181\n",
      "[epoch 374] loss: 7.387\n",
      "[epoch 375] loss: 6.917\n",
      "[epoch 376] loss: 7.038\n",
      "[epoch 377] loss: 6.811\n",
      "[epoch 378] loss: 6.832\n",
      "[epoch 379] loss: 6.812\n",
      "[epoch 380] loss: 6.860\n",
      "[epoch 381] loss: 6.326\n",
      "[epoch 382] loss: 6.426\n",
      "[epoch 383] loss: 6.062\n",
      "[epoch 384] loss: 6.333\n",
      "[epoch 385] loss: 6.820\n",
      "[epoch 386] loss: 6.544\n",
      "[epoch 387] loss: 6.972\n",
      "[epoch 388] loss: 6.612\n",
      "[epoch 389] loss: 7.194\n",
      "[epoch 390] loss: 6.959\n",
      "[epoch 391] loss: 6.959\n",
      "[epoch 392] loss: 6.107\n",
      "[epoch 393] loss: 6.158\n",
      "[epoch 394] loss: 6.150\n",
      "[epoch 395] loss: 6.742\n",
      "[epoch 396] loss: 6.696\n",
      "[epoch 397] loss: 6.147\n",
      "[epoch 398] loss: 6.070\n",
      "[epoch 399] loss: 6.166\n",
      "[epoch 400] loss: 5.917\n",
      "[epoch 401] loss: 5.843\n",
      "[epoch 402] loss: 6.138\n",
      "[epoch 403] loss: 6.069\n",
      "[epoch 404] loss: 6.120\n",
      "[epoch 405] loss: 5.815\n",
      "[epoch 406] loss: 5.755\n",
      "[epoch 407] loss: 5.804\n",
      "[epoch 408] loss: 5.823\n",
      "[epoch 409] loss: 5.819\n",
      "[epoch 410] loss: 5.637\n",
      "[epoch 411] loss: 6.149\n",
      "[epoch 412] loss: 5.995\n",
      "[epoch 413] loss: 5.880\n",
      "[epoch 414] loss: 5.787\n",
      "[epoch 415] loss: 5.549\n",
      "[epoch 416] loss: 5.661\n",
      "[epoch 417] loss: 5.337\n",
      "[epoch 418] loss: 5.367\n",
      "[epoch 419] loss: 5.342\n",
      "[epoch 420] loss: 5.109\n",
      "[epoch 421] loss: 5.027\n",
      "[epoch 422] loss: 5.072\n",
      "[epoch 423] loss: 5.104\n",
      "[epoch 424] loss: 5.246\n",
      "[epoch 425] loss: 5.186\n",
      "[epoch 426] loss: 4.857\n",
      "[epoch 427] loss: 4.964\n",
      "[epoch 428] loss: 4.987\n",
      "[epoch 429] loss: 5.646\n",
      "[epoch 430] loss: 5.252\n",
      "[epoch 431] loss: 4.867\n",
      "[epoch 432] loss: 4.769\n",
      "[epoch 433] loss: 5.598\n",
      "[epoch 434] loss: 4.663\n",
      "[epoch 435] loss: 4.649\n",
      "[epoch 436] loss: 4.760\n",
      "[epoch 437] loss: 5.118\n",
      "[epoch 438] loss: 4.907\n",
      "[epoch 439] loss: 4.469\n",
      "[epoch 440] loss: 4.254\n",
      "[epoch 441] loss: 5.173\n",
      "[epoch 442] loss: 4.976\n",
      "[epoch 443] loss: 4.760\n",
      "[epoch 444] loss: 4.716\n",
      "[epoch 445] loss: 4.715\n",
      "[epoch 446] loss: 4.672\n",
      "[epoch 447] loss: 4.954\n",
      "[epoch 448] loss: 4.494\n",
      "[epoch 449] loss: 4.606\n",
      "[epoch 450] loss: 4.229\n",
      "[epoch 451] loss: 4.231\n",
      "[epoch 452] loss: 4.269\n",
      "[epoch 453] loss: 4.052\n",
      "[epoch 454] loss: 4.057\n",
      "[epoch 455] loss: 4.079\n",
      "[epoch 456] loss: 3.841\n",
      "[epoch 457] loss: 3.957\n",
      "[epoch 458] loss: 3.899\n",
      "[epoch 459] loss: 3.833\n",
      "[epoch 460] loss: 4.412\n",
      "[epoch 461] loss: 4.779\n",
      "[epoch 462] loss: 4.390\n",
      "[epoch 463] loss: 4.597\n",
      "[epoch 464] loss: 4.550\n",
      "[epoch 465] loss: 5.199\n",
      "[epoch 466] loss: 4.575\n",
      "[epoch 467] loss: 4.197\n",
      "[epoch 468] loss: 3.940\n",
      "[epoch 469] loss: 4.104\n",
      "[epoch 470] loss: 4.199\n",
      "[epoch 471] loss: 3.906\n",
      "[epoch 472] loss: 3.620\n",
      "[epoch 473] loss: 3.510\n",
      "[epoch 474] loss: 3.906\n",
      "[epoch 475] loss: 3.687\n",
      "[epoch 476] loss: 3.430\n",
      "[epoch 477] loss: 3.503\n",
      "[epoch 478] loss: 3.449\n",
      "[epoch 479] loss: 3.644\n",
      "[epoch 480] loss: 3.442\n",
      "[epoch 481] loss: 3.269\n",
      "[epoch 482] loss: 3.400\n",
      "[epoch 483] loss: 3.938\n",
      "[epoch 484] loss: 4.102\n",
      "[epoch 485] loss: 3.959\n",
      "[epoch 486] loss: 3.911\n",
      "[epoch 487] loss: 3.812\n",
      "[epoch 488] loss: 3.751\n",
      "[epoch 489] loss: 3.453\n",
      "[epoch 490] loss: 3.332\n",
      "[epoch 491] loss: 3.165\n",
      "[epoch 492] loss: 3.006\n",
      "[epoch 493] loss: 2.936\n",
      "[epoch 494] loss: 2.781\n",
      "[epoch 495] loss: 2.769\n",
      "[epoch 496] loss: 3.272\n",
      "[epoch 497] loss: 3.003\n",
      "[epoch 498] loss: 3.139\n",
      "[epoch 499] loss: 3.021\n",
      "[epoch 500] loss: 2.989\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "!python train_cifar10.py -e 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演習4.2.8\n",
    "accuracy_cifar.pngからモデル100が良い最も精度が良かったため、これを使ってテストを行う(これ以降少し精度が上がっている部分もあるが、これは過学習であると考える。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "# Minibatch-size: 100\n",
      "\n",
      "Accuracy of airplane : 45 %\n",
      "Accuracy of automobile : 20 %\n",
      "Accuracy of  bird : 15 %\n",
      "Accuracy of   cat : 25 %\n",
      "Accuracy of  deer : 30 %\n",
      "Accuracy of   dog : 35 %\n",
      "Accuracy of  frog : 60 %\n",
      "Accuracy of horse : 40 %\n",
      "Accuracy of  ship : 55 %\n",
      "Accuracy of truck : 45 %\n",
      "Accuracy : 37.000 %\n"
     ]
    }
   ],
   "source": [
    "!python test_cifar10.py -m result/model_100"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai-experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
